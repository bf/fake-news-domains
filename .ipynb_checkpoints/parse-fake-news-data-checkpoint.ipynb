{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News URL Datasets\n",
    "\n",
    "There exist lists of fake news domains from three different data sources: **Politifacts**, **OpenSourceGroup**, and **Wikipedia**. \n",
    "\n",
    "In order to reliably identify fake news in our social media research studies, we want to reproducibly generate a data set of fake news domains from each of these data sources.\n",
    "\n",
    "Therefore we download and process the raw data from each data source in this python notebook, in order to generate unique lists of fake news domains for different research questions.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "1. Download the different fake news URL datasets\n",
    "\n",
    "- Clean up data & remove duplicates\n",
    "\n",
    "- Compare different datasets\n",
    "\n",
    "- Output as .txt files\n",
    "\n",
    "\n",
    "## 1) Download fake news URL datasets\n",
    "\n",
    "The datasets have already been downloaded to this folder. The data sources are documented in the readme text files.\n",
    "\n",
    "\n",
    "## 2) Clean up data & remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "From OpenSourceGroup:\n",
      "  - 833 entries in opensourcesgroup-raw-16Apr2020.csv\n",
      "  => unique fake news domains:\t 824\n",
      "\n",
      "From Politifacts:\n",
      "  - 327 entries in politifacts-raw-16Apr2020.csv\n",
      "  => unique fake news domains:\t 325\n",
      "\n",
      "From Wikipedia:\n",
      "  - 82 entries in wikipedia-fake-news-raw-16Apr2020.csv\n",
      "  - 18 entries in wikipedia-fake-news-usa-raw-16Apr2020.csv\n",
      "  => unique fake news domains:\t 88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_sources = {\n",
    "    \"OpenSourceGroup\": [\"opensourcesgroup-raw-16Apr2020.csv\"],\n",
    "    \"Politifacts\": [\"politifacts-raw-16Apr2020.csv\"],\n",
    "    \"Wikipedia\": [\"wikipedia-fake-news-raw-16Apr2020.csv\", \"wikipedia-fake-news-usa-raw-16Apr2020.csv\"]\n",
    "}\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# function to clean up our domains\n",
    "def clean_up_entry(str_domain_or_link):\n",
    "    # if domain is google.com/foo then we only want google.com\n",
    "    for sep in ['/', '\\\\', '#', '?']:\n",
    "        if sep in str_domain_or_link:\n",
    "            str_domain_or_link = str_domain_or_link.split(sep)[0]\n",
    "            \n",
    "    # make everything lowercase\n",
    "    domain = str_domain_or_link.lower()\n",
    "                \n",
    "    # replace www in front of domains\n",
    "    domain = domain.replace(\"www.\", \"\")\n",
    "    \n",
    "    return domain\n",
    "\n",
    "# read all downloaded files into our data structure\n",
    "for source in data_sources.keys():\n",
    "    print(\"\\nFrom\", source + \":\")\n",
    "    for csv in data_sources[source]:\n",
    "        # open + read csv file into pandas dataframe\n",
    "        df = pd.read_csv(\"data/\" + source + \"/\" + csv)\n",
    "        \n",
    "        # domains are always in first column of csv\n",
    "        first_column = list(df[df.columns[0]])\n",
    "        \n",
    "        print(\"  -\", len(first_column), \"entries in\", csv)\n",
    "        \n",
    "        # store the data (merge with other data for same source if needed)\n",
    "        if source in dfs.keys():\n",
    "            dfs[source].extend(first_column)\n",
    "        else:\n",
    "            dfs[source] = first_column\n",
    "    \n",
    "    # apply cleanup function to all entries\n",
    "    dfs[source] = [clean_up_entry(entry) for entry in dfs[source]]\n",
    "    \n",
    "    # remove duplicate entries\n",
    "    dfs[source] = set(sorted(dfs[source]))\n",
    "    \n",
    "    print(\"  => unique fake news domains:\\t\", len(dfs[source]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Compare different datasets\n",
    "\n",
    "Now that we have parsed the data, cleaned it up and removed the duplicates, we want to compare the datasets.\n",
    "\n",
    "We want to find out **common fake news domains accross all datasets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common fake news domains across all data sets: 31\n",
      "\n",
      "Total number of fake news domains: 1013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# calculate list of common fake news domains across all datasets\n",
    "common_fake_news_domains_across_all_datasets = reduce(set.intersection, dfs.values())\n",
    "\n",
    "print(\"Common fake news domains across all data sets:\",  len(common_fake_news_domains_across_all_datasets))\n",
    "print()\n",
    "#for domain in common_fake_news_domains_across_all_datasets:\n",
    "#    print(\"  -\", domain)\n",
    "\n",
    "\n",
    "# calculate list of all fake news domains\n",
    "all_fake_news_domains = reduce(set.union, dfs.values())\n",
    "\n",
    "print(\"Total number of fake news domains:\", len(all_fake_news_domains))\n",
    "print()\n",
    "#for domain in all_fake_news_domains:\n",
    "#    print(\"  -\", domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Output list of fake news domains as .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files:\n",
      "\n",
      " - fake-news-domains-OpenSourceGroup-Apr2020.csv  (824 entries)\n",
      " - fake-news-domains-Politifacts-Apr2020.csv  (325 entries)\n",
      " - fake-news-domains-Wikipedia-Apr2020.csv  (88 entries)\n",
      " - fake-news-domains-intersection-Apr2020.csv  (31 entries)\n",
      " - fake-news-domains-all-Apr2020.csv  (1013 entries)\n"
     ]
    }
   ],
   "source": [
    "# store data for saving\n",
    "dfs[\"intersection\"] = common_fake_news_domains_across_all_datasets\n",
    "dfs[\"all\"] = all_fake_news_domains\n",
    "\n",
    "# store all results in csv files\n",
    "print (\"Saving files:\\n\")\n",
    "\n",
    "outputdir = \"output/\"\n",
    "for key in dfs:\n",
    "    filename = \"fake-news-domains-\" + key + \"-Apr2020.csv\"\n",
    "    pd.DataFrame({\"fake_news_domain\": sorted(list(dfs[key])) }).to_csv(outputdir + filename, header=False, index=False)\n",
    "    print(\" -\", filename, \" (\" + str(len(dfs[key])) + \" entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
